{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleanup notebook\n",
    "This notebook is meant as both cleanup tool for the original dataset from [Zenodo](https://zenodo.org/record/5092309#.ZGumqxlByDQ) and as a documentation on what was changed if one uses already cleaned up dataset (most likely currently hosted privately on google cloud storage instance).\n",
    "\n",
    "If using the notebook as cleanup tool, download original dataset fromm [Zenodo](https://zenodo.org/record/5092309#.ZGumqxlByDQ). You can optionally calculate the md5 checksum to verify that the data was downloaded without an error:\n",
    "```bash\n",
    "$ wget --output-document gwhd_2021.zip https://zenodo.org/record/5092309/files/gwhd_2021.zip?download=1 && uznip gwhd_2021.zip\n",
    "$ python3 data_integrity.py <Path to the dataset> <original dataset MD5>\n",
    "```\n",
    "Then export environment variable **DATASET_ROOT_DIR** with path where the dataset was placed and then run this notebook. Remember to run the notebook from the terminal where **DATASET_ROOT_DIR** was exported!\n",
    "\n",
    "If using this notebook as documentation, there is no need to run it as the data should already be cleaned up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking environment variables...\n",
      "Environment variables exist.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(\"Checking environment variables...\")\n",
    "assert 'PROJ_PATH' in os.environ\n",
    "assert 'YOLOV7_ROOT_DIR' in os.environ\n",
    "assert 'DATASET_MD5' in os.environ\n",
    "assert 'ORIGINAL_DATASET_MD5' in os.environ\n",
    "assert 'DATASET_ROOT_DIR' in os.environ\n",
    "assert 'DATA_BUCKET' in os.environ\n",
    "print(\"Environment variables exist.\")\n",
    "\n",
    "DATASET_ROOT_DIR = os.environ['DATASET_ROOT_DIR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "\"\"\"\n",
    "All images in the dataset have .png extension, but some of them are actually .jpg files.\n",
    "It probably is not a problem during the training, but default image viewer is not opening these\n",
    "files correctly.\n",
    "\n",
    "Convert them all to actuall .png files.\n",
    "\"\"\"\n",
    "images_dir = f'{DATASET_ROOT_DIR}/images'\n",
    "for img_name in tqdm.tqdm(os.listdir(images_dir)):\n",
    "    try:\n",
    "        img = Image.open(img_name)\n",
    "        img.save(img_name, format='PNG')\n",
    "    except OSError as e:\n",
    "        print(f\"Couldn't convert {img_name} due to {e}, skipping...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_df = pd.read_csv(f'{DATASET_ROOT_DIR}/competition_train.csv')\n",
    "test_df = pd.read_csv(f'{DATASET_ROOT_DIR}/competition_test.csv')\n",
    "val_df = pd.read_csv(f'{DATASET_ROOT_DIR}/competition_val.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/home/js/gwhd_2021/images/b11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# Image b11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png is corrupted so we remove it\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m# It throws error that the file is truncated. It opens normally in default image viewer but not in imagemagick\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m os\u001b[39m.\u001b[39;49mremove(os\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(DATASET_ROOT_DIR, \u001b[39m'\u001b[39;49m\u001b[39mimages\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39m'\u001b[39;49m\u001b[39mb11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png\u001b[39;49m\u001b[39m'\u001b[39;49m))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/home/js/gwhd_2021/images/b11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Image b11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png is corrupted so we remove it\n",
    "It throws error that the file is truncated. It opens normally in default image viewer but not in imagemagick\n",
    "\"\"\"\n",
    "os.remove(os.path.join(DATASET_ROOT_DIR, 'images', 'b11b3c68d79f4025ff7f542587ab91a67dfe55be69d1fb63db4bcbcb108284a9.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset duplicates:\n",
      " ['d88963636d49127bda0597ef73f1703e92d6f111caefc44902d5932b8cd3fa94.png'\n",
      " '1961bcf453d5b2206c428c1c14fe55d1f26f3c655db0a2b6a83094476e8edb5b.png']\n",
      "Test dataset duplicates:\n",
      " ['da9846512ff19b8cd7278c8c973f75d36de8c4eb4e593b8285f6821aae1f4203.png']\n",
      "Val dataset duplicates:\n",
      " []\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check if there are duplicates in the subsets\n",
    "\"\"\"\n",
    "train_df_dups = train_df.loc[train_df['image_name'].duplicated(), 'image_name'].unique()\n",
    "test_df_dups = test_df.loc[test_df['image_name'].duplicated(), 'image_name'].unique()\n",
    "val_df_dups = val_df.loc[val_df['image_name'].duplicated(), 'image_name'].unique()\n",
    "print(f\"Train dataset duplicates:\\n {train_df_dups}\")\n",
    "print(f\"Test dataset duplicates:\\n {test_df_dups}\")\n",
    "print(f\"Val dataset duplicates:\\n {val_df_dups}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Drop these duplicates\n",
    "\"\"\"\n",
    "train_df.drop_duplicates(subset=['image_name'], inplace=True)\n",
    "test_df.drop_duplicates(subset=['image_name'], inplace=True)\n",
    "val_df.drop_duplicates(subset=['image_name'], inplace=True)\n",
    "df = pd.concat([train_df, test_df, val_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 0 leaks in the dataset\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Check if there is leak between train, test and val sets. At this point we are sure that\n",
    "there are no duplicates in each subset.\n",
    "\"\"\"\n",
    "import numpy as np\n",
    "leaks = np.array([])\n",
    "if not df['image_name'].nunique() == df.shape[0]:\n",
    "    # This are the error files that camee up during symlink creation in yolo format conversion\n",
    "    leaks = df.loc[df.duplicated(subset=['image_name'])]['image_name']\n",
    "    leaks = leaks.reset_index(drop=True)\n",
    "print(f\"Found {leaks.shape[0]} leaks in the dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicates boxes:  [[770 629 813 690]]\n",
      "New:  8 141 116 271;8 382 62 435;8 573 94 678;21 260 66 370;23 389 73 542;65 237 90 268;80 992 161 1024;110 298 161 390;136 733 195 810;191 39 235 113;209 256 349 356;237 371 269 419;264 360 376 420;291 148 350 216;298 17 397 72;315 489 459 574;375 234 435 289;378 755 468 897;404 704 488 829;420 357 525 403;504 398 566 446;564 610 608 806;571 323 677 378;582 163 728 237;593 249 662 320;615 10 673 95;625 653 671 718;697 434 808 486;756 741 782 774;763 782 813 832;763 915 821 1024;770 629 813 690;789 1 910 49;795 104 914 282;882 422 928 540;899 261 1006 311;930 149 977 205;930 412 980 474;941 807 977 849;959 474 1016 541\n",
      "Duplicates boxes:  [[ 74 370 146 484]]\n",
      "New:  0 62 52 138;0 250 80 358;0 756 40 824;74 370 146 484;78 802 248 892;82 0 166 62;104 966 194 1024;110 184 226 256;142 872 186 914;144 760 300 812;148 488 238 562;216 916 316 1024;222 896 346 976;224 424 258 450;278 0 446 34;300 366 384 468;426 738 500 832;444 56 612 186;452 208 582 286;474 896 596 1024;490 240 624 346;548 878 614 928;566 896 682 994;584 560 646 632;588 646 722 704;634 994 694 1024;702 362 784 438;732 260 852 334;734 198 930 268;754 142 862 220;814 256 916 302;846 286 936 400;894 34 972 118;894 748 1024 848;948 158 1024 210;956 480 1012 530;990 600 1024 666;990 962 1022 1012;992 712 1024 764\n",
      "Duplicates boxes:  [[ 977  529 1023  571]]\n",
      "New:  0 0 369 69;0 84 200 161;0 134 240 251;0 784 22 808;4 730 177 828;9 793 358 882;129 32 410 304;169 904 486 992;186 576 431 628;217 552 425 597;237 768 469 860;261 572 595 693;272 981 457 1023;274 365 399 494;286 442 495 550;296 289 541 535;324 829 738 973;368 294 409 559;369 69 446 273;370 724 598 846;380 977 652 1023;385 438 577 526;385 660 522 718;424 153 570 202;424 626 669 668;446 918 566 976;454 100 539 469;457 818 658 911;474 757 716 909;506 184 820 298;520 8 594 148;525 405 595 450;536 150 828 302;541 0 643 32;544 336 878 386;562 0 824 133;578 116 968 197;606 289 984 359;649 381 689 449;649 822 941 1023;665 0 985 168;728 676 774 834;781 946 870 1023;786 604 918 990;837 58 1023 146;861 168 991 426;884 760 1024 1024;901 72 1023 216;904 189 980 246;906 749 992 1023;936 244 1024 310;952 670 1024 818;977 529 1023 571\n",
      "Duplicates boxes:  [[693   0 749  43]]\n",
      "New:  0 241 42 296;0 602 34 664;17 404 55 454;30 965 80 1007;54 445 87 476;66 0 274 46;95 816 134 879;131 341 232 411;141 623 295 676;148 390 239 471;173 512 315 630;178 973 264 1024;191 466 275 581;191 914 262 982;197 104 297 226;233 183 296 231;240 610 280 642;250 554 362 590;251 402 305 439;254 223 312 330;274 416 419 554;288 0 357 50;293 108 365 148;293 212 360 300;310 116 468 221;310 161 374 230;318 683 356 733;319 933 442 987;321 384 517 456;343 358 402 411;351 83 431 208;361 202 443 302;362 727 468 884;400 206 447 265;411 576 466 611;415 41 545 102;421 534 578 605;425 1002 548 1024;438 949 535 1003;441 765 643 813;453 544 518 640;453 576 547 665;455 70 518 194;462 0 526 45;463 226 580 288;465 836 563 888;513 79 545 105;518 9 608 106;518 277 560 344;539 157 615 220;539 453 707 542;546 103 674 145;564 246 680 315;581 808 620 842;616 904 675 943;617 386 695 431;632 239 685 359;664 87 725 150;666 765 710 926;693 0 749 43;694 521 834 656;695 45 785 132;698 631 790 726;715 772 760 817;730 410 793 478;743 510 831 553;745 0 791 62;756 856 796 903;771 870 837 944;777 445 942 613;794 416 882 446;797 0 835 28;804 171 909 262;818 381 871 491;825 185 861 253;851 307 922 340;857 948 941 1024;869 0 927 59;870 492 903 518;891 464 951 544;907 133 977 375;934 418 1023 453;934 923 974 1002;949 0 1024 74;966 333 1024 413;970 997 1024 1024;981 575 1024 675;995 527 1024 574;1002 861 1024 903\n",
      "Duplicates boxes:  [[531 353 553 374]]\n",
      "New:  0 466 85 537;13 205 48 258;17 30 85 132;38 327 67 357;45 609 108 665;51 339 112 457;57 664 106 730;104 208 137 243;113 71 163 117;114 289 154 349;122 820 169 872;125 417 159 461;141 220 175 258;151 488 172 507;155 415 186 449;156 180 192 235;156 253 198 297;157 89 205 142;166 0 236 40;166 673 183 719;169 507 218 566;172 431 214 507;178 134 240 188;210 185 247 229;221 672 273 726;222 288 251 317;238 263 269 304;242 735 266 760;247 149 301 229;250 421 291 448;272 254 295 298;283 480 325 523;309 451 345 478;328 233 363 280;342 714 466 841;347 896 411 944;376 649 480 705;380 498 438 571;387 814 421 849;389 772 455 839;410 366 454 428;413 907 458 964;417 431 480 519;434 564 461 589;465 631 493 684;470 27 508 77;471 530 509 574;474 583 537 659;476 699 596 758;493 911 534 961;499 109 542 177;505 982 591 1007;506 316 540 361;518 312 569 363;519 228 555 293;523 143 569 176;531 353 553 374;531 466 585 553;547 211 599 263;553 252 626 320;555 302 611 347;559 345 590 388;599 336 628 383;600 425 629 477;605 199 638 297;636 612 730 655;653 79 686 118;656 37 693 78;693 976 837 1024;704 85 765 180;717 604 868 683;727 660 813 695;738 399 936 520;744 937 774 968;782 906 828 949;804 774 866 837;814 850 845 901;829 919 869 970;852 620 892 683;869 504 905 556;869 913 906 965;873 683 932 736;895 482 933 532;898 417 947 500;902 525 946 571;904 939 1011 967;919 706 947 734;936 305 985 360;942 432 961 510;962 336 1011 384;976 208 1024 298;1000 888 1024 924;1011 239 1024 288\n",
      "Duplicates boxes:  [[  1 190  34 217]]\n",
      "New:  0 309 62 346;1 190 34 217;11 288 51 363;18 376 52 430;23 164 65 193;88 172 131 204;92 886 126 1024;119 162 232 194;123 965 213 1013;137 1009 209 1024;139 698 189 973;142 35 186 73;168 196 257 228;175 792 204 819;208 828 261 955;211 1 295 88;211 772 243 829;230 395 266 430;297 0 340 17;374 772 534 951;400 675 454 719;415 291 555 348;457 743 512 798;466 476 569 591;485 179 614 217;515 691 559 733;578 294 625 348;581 81 628 157;581 452 631 515;621 370 670 397;628 648 655 675;654 849 686 925;657 911 720 979;661 118 725 166;661 987 725 1018;682 815 746 862;727 684 772 732;745 189 831 229;770 859 1014 989;853 264 888 331;887 660 927 692;922 208 990 253;929 606 990 635;954 325 979 365;967 528 1023 620;994 633 1023 663\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\"\"\"\n",
    "Check if there are any duplicate bboxes in the dataset per image. If there are remove them.\n",
    "\"\"\"\n",
    "for row in df.iterrows():\n",
    "    if row[1]['BoxesString'] == 'no_box':\n",
    "        continue\n",
    "    bboxes = row[1]['BoxesString'].split(';')\n",
    "    bboxes = [bbox.split(' ') for bbox in bboxes]\n",
    "    bboxes = [[int(i) for i in bbox] for bbox in bboxes]\n",
    "    bboxes = np.array(bboxes, dtype=int)\n",
    "    uniques, count = np.unique(bboxes, axis=0, return_counts=True)\n",
    "    duplicate_bboxes = uniques[count > 1]\n",
    "    if duplicate_bboxes.size > 0:\n",
    "        print(f\"Duplicate boxes found:\\n{duplicate_bboxes}\")\n",
    "        new_bboxes_string = ';'.join(' '.join(unique_bbox.astype(str)) for unique_bbox in uniques)\n",
    "        row[1]['BoxesString'] = new_bboxes_string\n",
    "        print(f\"New duplicate-free BoxesString:\\n{new_bboxes_string}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "masters-pt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a7f3476f6f4e819aacaf6e07d5335015c6631c94833f82990f5f57300cd500a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
